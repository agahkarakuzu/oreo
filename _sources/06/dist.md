Research disemmination
=========================================================

```{admonition} Issue
Through the whole research cycle a range of outputs far beyond publications are produced, and each of them can have different levels of reproducibility and openness. For shared resources to be useful, they need to follow the FAIR principles {cite:p}`Wilkinson2016-fm`, to ensure they are: Findable (e.g., using persistent identifiers, such as Digital Object Identifiers (DOI) or Research Resource Identifiers (RRIDs), and described with rich metadata indexed in a searchable resource), Accessible (e.g., shared in public repositories, under open access or controlled access depending on regulations, so they can be retrievable by their identifier using standardized communication protocols), Interoperable (e.g., following a common standard for organization and vocabulary), and Reusable (e.g., richly described, with detailed provenance and an appropriate license). Indeed, without a license, materials (data, code, etc.) become unusable by the community due to the lack of permission and conditions for reuse, copy, modification, or distribution. Therefore, consenting through a license is essential for any material to be publicly shared.
```

```{admonition} See also
:class: seealso
A useful general purpose resource, beyond neuroimaging, to find practical guidelines on reproducible research, project design, communication, collaboration, and ethics is The Turing Way (TTW, {cite:p}`The_Turing_Way_Community2019-be`, see the [resources table](../09/table.md)). TTW is an open collaborative community-driven project, aiming to make data science accessible and comprehensible to ensure more reproducible and reusable projects.
```

```{admonition} What do we provide
TBE.
```

```{figure} ../figures/fig5.png
---
name: fig5
---
Disemmination [^footnote5].
```


:::{dropdown} {fa}`share-alt` 6.1 Data sharing
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s61)=
Making data available to the community is important for reproducibility, allows more scientific knowledge to be obtained from the same number of participants (animal or human), and also enables scientists to learn and teach others to reuse data, develop new analysis techniques, advance scientific hypotheses, and combine data in mega- or meta-analyses {cite:p}`Laird2021-nb,Madan2021-bo,Poldrack2014-lr`. Moreover, the willingness to share has been shown to be positively related to the quality of the study {cite:p}`Aczel2021-hu,Wicherts2011-gd`. Because of the many advantages data sharing brings to the scientific community {cite:p}`Milham2018-pr`, more and more journals and funding agencies are requiring scientists to make their data public (curated and archived with a public record, but controlled access) or open (public data with uncontrolled access) upon the completion of the study, as long as it does not compromise participants’ privacy, legal regulations, or the ethical agreement between the researcher and participants (see {ref}`Section 2.3 <s23>` and [Section 7](../07/conclusions.md)).

For data to be interoperable and reusable, it should be organized following an accepted standard, such as BIDS ({ref}`Section 4.1 <s41>`) and with at least a minimal set of metadata. Free data-sharing platforms are available for publicly sharing neuroimaging data, such as OpenNeuro {cite:p}`Markiewicz2021-mi`, brainlife {cite:p}`Avesani2019-vi`, GIN (G-Node Infrastructure), Distributed Archives for Neurophysiology Data Integration (DANDI), International Neuroimaging Data-Sharing Initiative (INDI), NeuroImaging Tools & Resources Collaborator (NITRC), etc. (see the [resources table](../09/table.md)). Data could also be shared on institutional and funder archives such as the National Institute of Mental Health Data Archive (NDA); on dedicated repositories, such as the the Cambridge Centre for Ageing and Neuroscience, Cam-CAN {cite:p}`Shafto2014-pe,Taylor2017-nq` or The Open MEG Archive, OMEGA {cite:p}`Niso2016-gh`; or on generic archives that are not neuroscience or neuroimaging specific, such as figshare, GitHub, the Open Science Framework, and Zenodo. If allowed by the law and participants’ consent (see {ref}`Section 2.3 <s23>`), data sharing can be made open, or at least public.

Once curated and archived, data can further benefit the individual researcher, for example by adding them to the scientific literature in the form of data descriptors. Such an article type is not intended to communicate findings or interpretations but rather to provide detailed information about how the dataset was collected, what it includes and how it could be used, along with the shared data. In addition, an “open science badge” for data sharing is available in an increasing number of scientific journals {cite:p}`Kidwell2016-sw` and some prizes are also available as a recognition for such efforts (e.g., OHBM’s Data Sharing prize).

It is important to note that there are unresolved issues with international data sharing that make it hard for some researchers to share their neuroimaging data. The two main issues are compliance with privacy regulations and credit. Privacy regulations can differ tremendously between cultural, legal, and ethical regions and these differences have an impact on whether certain data can be shared (e.g unprocessed MRI images) and if so, in which way (e.g. freely or after signing a data user agreement). Platforms that do not provide the necessary privacy protection mechanisms may not be an option for storing data from a jurisdiction that requires them. There is an ongoing discussion of the topic (see e.g {cite:p}`Eke2022-nc,Jwa2022-dq`) and solutions such as EBRAINs {cite:p}`Amunts2016-hu,Amunts2019-nk`, a GDPR compatible data sharing platform in the Human Brain Project, as well as GDPR compatible consent forms (see Open Brain Consent, {ref}`Section 2.3 <s23>`) are being developed. It can be expected that in the future data sharing procedures will undergo transformations as privacy laws will develop in more jurisdictions towards GDPR type laws (e.g. the Consumer Privacy Protection Act in Canada, or the California Consumer Privacy act). As for credit, sharing data with a DOI, or as a data paper when appropriate, allows the researchers to receive academic credit via citations.
:::


:::{dropdown} {fa}`eye` 6.2 Methodological transparency
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s62)=
Documenting performed analysis steps is key for reproducing studies’ results. For studies containing a small number of procedures, the methods section of an article could detail them in full length. This is, however, often not the case in current neuroimaging studies, where authors may need to summarize content in order to fit into the designated space, probably omitting relevant details. Therefore, the programming code itself becomes the most accurate source of the exact analysis steps performed on the data, and is anyway needed for reproducibility. Thus, it needs to be organized and clear (see recommendations suggested by {cite:p}`Sandve2013-kn,Van_Vliet2020-xd,Wilson2017-td`, otherwise, results may not be reproducible, or even correct {cite:p}`Casadevall2014-wf,Pavlov2021-tg`. It should be noted that sharing an imperfect code is still much better than not sharing at all {cite:p}`Barnes2010-rl,Gorgolewski2016-iy`.

While GUIs are a great and interactive way to learn to analyze data, one has to pay special attention to properly report the steps followed, as clicks are more difficult to be reported and reproduced than code {cite:p}`Pernet2015-jx`. Some toolboxes using GUIs keep track of data ‘history’ and ‘provenance’ (e.g., Brainlife, Brainstorm, EEGLAB, FSL’s Feat tool) facilitating this task. Efforts are being put to improve these features.

To ensure long-term preservation of the shared code, we suggest using version control systems such as Git, and social coding platforms such as GitHub in combination with an archival database for assigning permanent DOI to code served for research, for instance, Zenodo {cite:p}`Troupin2018-px`, brainlife.io/apps {cite:p}`Avesani2019-vi` or Software Heritage {cite:p}`Di_Cosmo2018-rb`. These platforms help keep a snapshot of the version of the code used for the paper published, allowing exact reproduction in case of later code updates.
:::

:::{dropdown} {fa}`filter` 6.3 Derived data sharing
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s63)=
Sharing data derivatives is perhaps the most critical yet most challenging aspect of data sharing in support of reproducible science. The BIDS standard provides a general description of common derivative data (e.g., preprocessed data and statistical maps) and is actively working towards extending advanced specific derivatives for the different neuroimaging modalities. Yet as of today, standards for the description of advanced derivatives (such as activation or connectivity maps, or diffusion measures) are not available or mature for wide use. As a result, to date, the community lacks clear guidance and tools on how derived data should be organized to maximize its reuse and to encompass its provenance, and where such data could be shared.

Solutions for sharing derivatives comprise a mixture of in-house and semi-standardized data-tracking and representation methods. Examples of data-derivative sharing are the high-profile, centralized projects, such as the Human Connectome Projects {cite:p}`Van_Essen2012-jg`, Ebrains of the Human Brain Project {cite:p}`Amunts2016-hu,Amunts2019-nk`, the UK-Biobank {cite:p}`Alfaro-Almagro2016-pj`, the NKI-Rockland sample {cite:p}`Nooner2012-hm`, and the Adolescent Brain Cognitive Development (ABCD; {cite:p}`Feldstein_Ewing2018-og`) to name a few (see the [resources table](../09/table.md)). These projects have developed project-specific solutions and in doing so also have provided a first-level implementation of what could be considered a data derivative standard. Yet, these projects are far from being open or community-developed as they must be centrally governed and mandated by the directives of the research plan.

As a result of the paucity of community-oriented standards, archives and software methods, sharing highly-processed neuroimaging data is still the frontier of reproducible science. One community-open archive for highly processed neuroimaging data derivatives is NeuroVault {cite:p}`Gorgolewski2015-if`. The archive accepts brain statistical maps derived from fMRI with the goal of being reused for meta-analytic studies. Data upload is open to researchers world-wide and the archive can accept brain maps submitted using most major formats but preferably using the NeuroImaging Data Model (NIDM; see {ref}`Section 4.2 <s42>`). Another interesting example for automated and standardized composition and sharing of derived data is TemplateFlow {cite:p}`Ciric2021-uw`, which provides an open and distributed framework for establishment, access, management, and vetting of reference anatomies and atlases.

 Another general neuroimaging platform, which allows lowering the barrier to sharing highly processed data derivatives, is Brainlife. Brainlife provides methods for publishing derivatives integrated with the data-processing applications used for generating results from the data via easy-to-use web interfaces for data-upload, processing, and publishing. Different licenses can be selected when publishing a record, allowing reuse of data and derivatives to other researchers (see sources that support the selection of an appropriate license listed in the [resources table](../09/table.md)).

Sharing lighter-weight data products such as tables and figures is easier using generic repositories (e.g., OSF, Figshare, Github or Zenodo) under, for example, CC-BY license. This allows authors to retain rights on the figures they created, and others to re-use their figures freely, either in other publications or for educational purposes, while giving credit to the originating team. Additional material, such as slide presentations, or supporting content, should be shared using accessible formats (e.g., image files, pdf, PowerPoint slides, Markdown, jupyter notebooks, etc) via online repositories or institutional platforms, with appropriate licenses to indicate how the work could be reused. Whenever possible, using platforms that ensure long-term preservation is recommended (e.g., Zenodo, FigShare, OSF). Using platforms that provide a DOI is particularly encouraged, because it ensures that the shared data would be identifiable in the future.
:::


:::{dropdown} {fa}`copy` 6.4 Publication of scientific results
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s64)=
Scientific papers are currently the most important means for disseminating research results. However, they should also be written with reproducibility in mind. Guidelines to improve reproducibility can support the writing. The OHBM Committee on Best Practices in Data Analysis and Sharing (COBIDAS), has been promoting best practices, including open science. Recommendations from the committees for MRI {cite:p}{Nichols2017-qb} and MEG and EEG {cite:p}`Pernet2020-zo` provide guidance on what to report. Other recent community efforts also led to guidelines for PET {cite:p}`Knudsen2020-nz` and EEG reporting (e.g., Agreed Reporting Template for EEG Methodology - International Standard (ARTEM-IS) {cite:p}`Styles2021-wj`). One tool that could help authors follow these guidelines while writing their report are their web-based apps (see the [resources table](../09/table.md)). For the data description and preprocessing aspects, some tools (pyBIDS, bids-matlab) or pipelines (fMRIPrep) can also generate reports automatically, and/or method templates are provided (see {ref}`Section 5.2 <s52>`). Exact description of methods is mandatory for reproducibility alongside detailed reporting of results.

In recent years, it has become very common in neuroimaging to publish papers as preprints, on servers such as bioRxiv, medRxiv, PsyArXiv or OSF (see the [resources table](../09/table.md)), prior to peer review in scientific journals. Preprints are publicly available, expedite the process of releasing new findings, and also, importantly, allow authors to get feedback on their paper from a broader audience prior to final publication {cite:p}`Moshontz2021-qj`. There are also initiatives for open community reviews of preprints, such as PREreview. Other initiatives have emerged to adapt to this paradigm shift, such as the recently launched Open Europe Research platform for publication and open peer review, which also includes the different outputs obtained throughout the research cycle (e.g., study protocol, data, methods and brief reports through the process) for research stemming from Horizon 2020 and Horizon Europe funding. In addition, novel publication formats have been developed, like [NeuroLibre](https://neurolibre.org) {cite:p}`Karakuzu:2022wq`, a preprint server to publish hybrid research objects including text, code, data, and runtime environment {cite:p}`DuPre2022-sh`. More traditional publishers have successfully partnered with companies such as CodeOcean {cite:p}`Cheifet2021-cr` to provide similar services.

Crucially, papers should be accessible to others, preferably to everyone. Many scientific publications are hidden behind paywalls, practically denying access from many people who could have gained from them. This is now slowly changing {cite:p}`Piwowar2018-ty`, with both researchers and funding agencies pushing towards open access, meaning that papers are fully open to all. Although the adoption of the concept of open access by major publishers is in itself a positive development, the way it was adopted could be considered arguable. For example, several journals considerably increased article processing charges {cite:p}`Budzinski2020-co,Khoo2019-xy`, increasingly excluding research produced in Low and Middle Income Countries from being published open access {cite:p}`Nabyonga-Orem2020-vt`. Additionally, some publishers implement massive tracking technology with the argument to protect their rights and offer the data or derivatives of them for sale, as a recent report published by the German Research Foundation criticizes (DFG 2021). This raises many questions, related, for example, to the influence publishers and their algorithms will have in the future on strategic decisions of science institutions and freedom of science.
:::

:::{dropdown} {fa}`book` 6.5 Beyond publication
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s65)=
The research lifecycle continues beyond paper publication. Disseminating scientific results to the broader scientific community and to the society in general is of utmost importance, to translate the newly acquired knowledge and give back to society. Oral and poster presentations at conferences contribute to the dissemination of results (including preliminary or intermediate results) within the scientific community, and also provide opportunities for feedback prior to publication. Workshops and other educational events contribute to expand the knowledge further and induce new communities. Popular and social media (e.g., press releases, interviews, podcasts, blog posts, twitter, facebook, linkedin, youtube, etc.) may reach an even wider and more heterogeneous audience. Different types of audiences may have different degrees of expertise and scientific knowledge, hence, for an effective communication, each of the outreach events should adapt accordingly (e.g., avoiding jargon and over interpretation, identifying your audience, promoting accessibility in content and language {cite:p}`Amano2021-wl`, and considering disabilities). See the TTW Guide for Communication for recommendations (https://the-turing-way.netlify.app/communication/communication.html). Slides presentations, and further outreach content should be shared FAIRly for higher impact (see {ref}`Section 6.3 <s63>`).

Hackathons - such as the Brainhacks in the neuroimaging community {cite:p}`Gau2021-rp` - typically offer times for “unconferences'' in which attendees can propose a short talk to present some work-in-progress, an open question, or any other topic they wish to discuss with other participants. This deviates from more typical conferences in which only well-polished, finalized results can be presented. Other initiatives, such as Neuromatch Academy, Neurohackademy, OHBM Open Science Room, and Brainhack school MLT facilitate open science and provide opportunities for researchers to learn and get hands-on experience with open science practices, and also to engage with other researchers in the community. Those hackathons and the related online communities are also well-known as kick-starters for the development of community tools and standards in which researchers and engineers from different labs join forces. As those tools and standards get shaped, typically in multi-lab collaborations, researchers get the chance to exchange their views and practices. Overall, such events, slowly but surely, help shape a research culture that is driven by open collaborative communities rather than single groups of researchers.
:::

:::{dropdown} {fa}`users` 6.6 Towards inclusive, diverse and community driven research
:title: bg-ch6 font-weight-bold
:animate: fade-in
(s66)=
Taken to the next level, the described developments and introduced tools provide an opportunity for a paradigm shift: rather than carrying out a study from inception to results and only then disseminating the findings to the community, researchers now get multiple opportunities to share their ideas and results as they are being developed. Scientific research can now become more transparent, inclusive and collaborative throughout the research cycle.

In collaborative projects, having a well defined Code of Conduct is critical to ensure a welcoming and inclusive space for everyone regardless of background or identity. Initiatives such as [TTW](http://doi.org/10.5281/zenodo.3233986) or the OHBM Conference have detailed Code of Conducts that can be of inspiration to adapt and use in new collaborative projects.

Community efforts should aim for equity, diversity and inclusivity. Unfortunately, as a reflection of our societies and cultures, the lack of these values is also prevalent in research and academia, contributing to gender bias, marginalization, and underrepresentation of racial, ethnic, and cultural minorities {cite:p}`Dworkin2020-yx,Hofstra2020-cx,Leslie2015-rv`. These inequalities are produced at all stages, even at the time of recruiting participant samples for experiments {cite:p}`Forbes2021-rs,Henrich2010-ew,Laird2021-nb` (see examples in the [Section 2](../02/ipe.md)). Importantly, observing equity, diversity, and inclusivity in the recruitment of study participants can also improve the quality of research results (e.g., {cite:p}`Baggio2013-ay`). Over the past years, the awareness and number of initiatives to mitigate bias and inequity at individual and institutional levels are growing {cite:p}`Levitis2021-rb,Llorens2021-yp,Malkinson2021-tg,Schreiweis2019-yc`. These aim to produce better research powered by a broader range of perspectives and ideas and to reduce the negative impact on the careers, work-life balance, and mental health of underrepresented groups. By providing open resources and promoting welcoming and inclusive spaces, we are also improving access to the tools, training and infrastructure which can facilitate reproducible research, which will accelerate discoveries, and ultimately, advance science.
:::


[^footnote5]: Sources: Icons from the Noun Project: Data Sharing by Design Circle; Share Code by Danil Polshin; Data by Nirbhay; Publication by Adrien Coquet; Broadcast by Amy Chiang; Logos: used with permission by the copyright holders.

:::{dropdown} References on this page
```{bibliography}
:filter: docname in docnames
:labelprefix: E
```
:::
