Study inception, planning, and ethics
=========================================================

```{admonition} Issue
Each individual decision from the beginning of the study will contribute to facilitate or hamper reproducibility.
```

```{admonition} What do we provide
In the current section, we will describe practices and tools for preparation, piloting, pre-registration, obtaining participants’ consent and ongoing quality control and assessment.
```

```{figure} ../figures/fig1.png
---
name: fig1
---
Study inception and planning [^footnote1].
```

:::{dropdown} {fa}`cogs` 2.1 Study preparation and piloting
:class-title: bg-ch2 font-weight-bold
:animate: fade-in
(s21)=
Research projects usually begin with descriptions of general, theoretical questions in documents such as grants or thesis proposals. Such foundations are essential but necessarily broad. When the project moves from proposal to implementation, these descriptions are translated into concrete protocols and stimuli, a process that can be streamlined by the incorporation of open procedures and comprehensive piloting. The promise is that the more preparation and piloting is conducted prior to data collection, the more likely it is that the project will be successful: that analyses of its data can contribute to answering its motivating ideas and questions {cite:p}`Strand:2021tm`.

Standard Operating Procedures (SOPs) can take different forms, and are powerful tools for planning, conducting, recording, and sharing projects. Ideally, SOPs describe the entire data collection procedure (e.g., recruitment, performing the experiment, data storage, preprocessing, quality control analyses), in sufficient detail for a reader to conduct the experiment themselves with minimal supervision, thereby contributing to reproducibility. SOPs may begin as an outline with vague descriptions, preferably during the pilot stage, and then become more detailed over time. For example, if a session is lost due to a button box signal failure, an image of its correct settings could be added to the SOPs. At the end of the project, its SOPs should be released along with its publications and datasets, to provide a source of answers for the detailed procedural information that may be needed for experiment replication or dataset reuse, but are not included in typical publications.

Many resources can assist with experiment planning and SOP creation. Documents and experiences from similar studies conducted locally are valuable, but should not be the only source of information during planning, since, for example, a procedure may be considered standard in one institution but not in another. Public SOPs can serve as examples, as can protocols published on specialized sites (e.g., Protocol Exchange, protocols.io, Nature Protocols; see Table S1). Best practices guides are now available for many imaging modalities (MRI: {cite:p}`Nichols2017-qb`; MEG/EEG: {cite:p}`Pernet2020-ep`; fNIRS: {cite:p}`Yucel2021-pk`; PET: {cite:p}`Knudsen2020-nz`). Open resources for stimulus presentations and behavioral data acquisition are also recommended to increase reproducibility (see {ref}`Section 3.2 <s32>`).

Piloting should be considered an integral part of the planning process. By “piloting” we mean the acquisition and evaluation of data prior to the collection of the actual experimental data; verifying the feasibility of the whole research workflow. While it is not a necessary prerequisite for reproducibility, it is a good scientific practice to produce higher quality research, and facilitates reproducibility via better documentation and SOPs. A piloting stage before starting data collection is important, not only for ensuring that the protocol will go smoothly when the first participant arrives, but also that the SOPs are complete and, critically, that the planned analyses can be carried out with the actual experimental data recorded. For example, pilot tests may be set up to confirm that the task produces the expected behavioral patterns (e.g., faster reaction time in condition A than B), that the log files are complete, and that image acquisition can be synchronized with stimulus presentation. Piloting should also include testing the data storage and retrieval strategies, which may include storing consent documents (see {ref}`Section 2.3 <s23>`), questionnaire responses, imaging data, and quality control reports. SOPs also prescribe how data will be organized, preferably according to a schema (e.g., the Brain Imaging Data Structure, see {ref}`Section 4.1 <s41>`). Data organization largely determines the efficient implementation of analysis pipelines and improved reproducibility, reusability, and shareability of the data and results.

Analyses of the pilot data are very important and can take several forms. One is to test for the effects of interest: establishing that the desired analyses can be performed and that the data quality and effect sizes are sufficient to produce valid and reproducible results (see {ref}`Section 2.2 <s22>` and see {ref}`2.4 <s24>`; for power estimation tools see the [resources table](../09/table.md)). A second type of pilot analysis is to establish tests for effects not of direct interest, but suitable for controls. As discussed further in section 2.4, positive control analyses involve strong, well-understood effects that must be present in a valid dataset. It is worth mentioning that well structured and documented openly available datasets (see [Section 4](../04/data.md)) could also serve for analysis piloting though they would lack the test for potential site specific technical issues. Simulations could also be used to ensure that the planned analysis is doable and valid.
:::


:::{dropdown} {fa}`plus-square` 2.2 Preregistration
:class-title: bg-ch2 font-weight-bold
:animate: fade-in
(s22)=
Pre-registration is the specification of the research plan in advance, prior to data collection or at least prior to data analysis {cite:p}`Nosek2018-ft`. Pre-registration usually includes the study design, the hypotheses and the analysis plan. It is submitted to a registry, resulting in a frozen time-stamped version of the research plan. Its main aim is to distinguish between hypothesis-testing (confirmatory) research and hypothesis-generating (exploratory) research. While both are necessary for scientific progress, they require different tests and the conclusions that can be inferred based on them are different {cite:p}`Nosek2018-yr`.

Registered reports is a relatively novel publishing format that can be seen as advanced pre-registration. This format is becoming very common, with a growing number of hundreds of participating journals {cite:p}`Hardwicke2018-fi,Chambers2019-ns`. In a registered report, a detailed pre-registration is submitted to a specific journal, including introduction, planned methods and potentially preliminary data. Then, it goes through peer review prior to data collection (or prior to data analysis in certain cases, for example for studies that rely on large-scale publicly shared data). If the proposed plan is approved following peer review, it receives an “in-principle acceptance”, indicating that if the researchers follow their accepted plan, and their conclusions fit their findings, their paper will be published. An in-principle accepted registered report is sometimes required to be additionally pre-registered. Recently, a platform for peer review of registered reports preprints was launched, named “Peer Community in registered reports” (see the [resources table](../09/table.md)).

There are many benefits to pre-registration, from the field to the individual level. Transparency with regard to the research plan, and whether an analysis is confirmatory or exploratory, increases the credibility of scientific findings. It helps to mitigate some of the effects of human biases on the scientific process, and reduces analytical flexibility, p-hacking {cite:p}`Simmons2011-zo` and hypothesizing after the results are known {cite:p}`Kerr1998-wf,Munafo2017-sm,Nature2015-cq,Nosek2019-zt`. There is initial evidence that the quality of pre-registered research is judged higher than in conventional publications {cite:p}`Soderberg2021-gd`. Nonetheless, it should be noted that pre-registration is not sufficient to fully protect against questionable research practices {cite:p}`Devezer2021-lk,Paul2021-zm,Rubin2020-ox` and their general impact will depend on the extent journals will implement them. Registered reports also mitigate publication bias by accepting papers based on hypothesis and methods, independently of the findings, and indeed it has been shown that pre-registration and registered reports lead to more published null findings {cite:p}`Allen2019-cb,Kaplan2015-ul,Scheel2020-lw` and report lower effect sizes {cite:p}`Allen2019-cb,Kaplan2015-ul,Scheel2020-lw`. For the individual researcher, registered reports with a two-stage review are an excellent example in which authors benefit from feedback on their methods before even starting data collection. They can help improve the research plan and spot mistakes in time, and provide assurance that the study will be published {cite:p}`Kidwell2016-sw,Wagenmakers2016-fo`. It should be noted, though, that registered reports require a significant time commitment, that is likely to pay off in the long-term but is not easily accommodated in many traditional project funding models.

While pre-registration is not the common practice yet, it is becoming more common over time {cite:p}`Nosek2018-ft` and requirements by journals and funding agencies are already changing. There are many available templates and forms for pre-registration, organized by discipline or study types, for example for fMRI and EEG (see the [resources table](../09/table.md)), and published guidelines for pre-registration in EEG are also available {cite:p}`Paul2021-zm`. There are different approaches with respect to what should be pre-registered. For instance, some believe it should be an exhaustive description of the study, including the background and justification for the research plan, while others believe it should be short and concise, including only the necessary details to reduce the likelihood of p-hacking and allowing reviewers to review it properly during the peer review process {cite:p}`Simmons2021-tc`. Pre-registration could also be more flexible and adaptive by pre-registering contingency plans or complex decision trees {cite:p}`Benning2019-te`.

Once researchers develop an idea and design a study, they can write and pre-register their research plan {cite:p}`Nosek2018-ft,Paul2021-zm`. Pre-registration could be performed following the piloting stage ({ref}`Section 2.1 <s21>`), but studies can be pre-registered irrespective of whether they include a piloting stage or not. There are many online registries where researchers can pre-register their study. The three most frequently used platforms are: (1) [OSF](https://osf.io), a platform that can also be used to share additional information about the study/project (such as data and code), with multiple templates and forms for different types of pre-registration, in addition to extensive resources about pre-registration and other open science practices; (2) [aspredicted.org](https://aspredicted.org), a simplified form for pre-registration (Simmons, Nelson, and Simonsohn 2021); and (3) [clinicaltrials.gov](https://clinicaltrials.gov), which is used for registration of clinical trials in the U.S. (see the [resources table](../09/table.md)).

Once the pre-registration is submitted, it can remain private or become public immediately, depending on the platform and the researcher’s preferences. Then, the researcher collects the data and executes the research plan. When writing the manuscript to report the study, the researcher is advised to include a link to the pre-registration, clearly and transparently describe and justify any deviation from the pre-registered plan and also report all registered analyses. Additional analyses to deepen some results or look into unexpected effects are encouraged, and are part of the normal scientific investigation. The added benefit of pre-registration is that such analyses do not need to reach pre-specified levels of significance because they are reported as exploratory.
:::

:::{dropdown} {fa}`share-alt` 2.3 Ethical review and data sharing plan
:class-title: bg-ch2 font-weight-bold
:animate: fade-in
(s23)=
The optimism of the scientific community about improving science by making all research assets open and transparent has to take into account privacy, ethics and the associated legal and regulatory needs for each institution and country. Whereas on the one hand sharing data (most often collected with public funds) is critical to advance science, on the other hand, sharing data can in some situations become infeasible to safeguard privacy. Data governance concerns the regulatory and ethical aspects of data management and sharing of data files, metadata and data-processing software (see sections {ref}`6.1 <s61>`, {ref}`6.2 <s62>` and {ref}`6.3 <s63>`). When data sharing crosses national borders, data governance is called International Data Governance (IDG). IDG depends on ethical, cultural and international laws.

Data sharing is beneficial for both reproducibility and the exploration and formulation of new hypotheses. Therefore, it is important to ensure, prior to data collection, that the collected data could be later shared. Consent forms for neuroimaging studies should contain, or be accompanied with, a specific section to allow data sharing. Such data sharing forms should include information about where and in what form the data will be shared, as well as who will have access and how restrictive this access will be. Data sharing forms should also make explicit how these factors determine to what extent a later withdrawal or editing of the data on the repository is possible. Additional requirements might be in place depending on the location where the research is being conducted. Given the international nature of the majority of neuroscience projects, IDG has become a priority {cite:p}`Eiss2020-dq`. Further work will be needed to implement an IDG approach that can facilitate research while protecting privacy and ethics. Specific recommendations on how to implement IDG have been proposed by {cite:p}`Eke2022-nc`.

Open and reproducible neuroimaging thus starts by (1) planning which data would be collected; (2) planning how these data would later be shared; (3) having ethical and legal clearance to share data; but also (4) the infrastructural means for this sharing (for more information about data sharing and available platforms, see [Section 6](../06/dist.md); for data governance see [Section 6](../04/data.md)). The informed consent should include a statement about the research goals, inform about the experimental procedures, its expected duration, potential risks, who can participate in a study, and potential conflicts of interest if they exist. It should also inform participants that participation is voluntary, and that the consent can be withdrawn at any time during the experiment without a disadvantage for the participant. If limited intervals for consent withdrawal exist, they should be clearly stated. Furthermore, it should also include a statement that the participant understood the information provided and agreed to participate. Clinical research may require adherence to additional, country specific regulations.

Since 2014 the Open Brain Consent project (see the [resources table](../09/table.md)), which was founded under the ReproNim project umbrella, provides examples and templates translated to multiple languages to help researchers prepare consent forms, including the recent development of an EU GDPR-compliant template (The Open Brain Consent working group 2021). When planning the recruitment procedures, it is important to aim for equity, diversity and inclusivity {cite:p}`Forbes2021-rs,Henrich2010-ew`, avoiding obtaining results that may not generalize to larger populations and improving the quality of research (e.g., {cite:p}`Baggio2013-ay`).
:::

:::{dropdown} {fa}`eye` 2.4 Looking at the data early and often: monitoring quality
:class-title: bg-ch2 font-weight-bold
:animate: fade-in
(s24)=
Inevitably, unexpected events and errors will occur during every experiment and in every part of the research workflow. These can take many forms, including dozing participants, hardware malfunction, data transfer errors, and mislabeled results files. As data progresses through the workflow, issues are likely to cascade and amplify, perhaps masking or mediating experimental effects, thereby damaging the reliability of the results. The impact of such surprises can range from the trivial and easily corrected to the catastrophic, rendering the collected data unusable or conclusions drawn from it invalid. Identifying issues and errors as early as possible is important to enable adding corrective measures to the protocol, but also because some issues are much easier to detect when the data are in a less-processed form. For example, a number of typical artifacts in anatomical MRI are known to be easier to identify in the background of the image and regions of no-interest {cite:p}`Mortamet2009-lk`, and can easily remain undetected if the first quality control check is set up after, e.g., brain extraction, which masks out non brain tissue. Thus, it is fundamental to pre-establish within the SOPs (section 2.1) the mechanisms set in place to ensure the quality of the study. There are several mechanisms available that help to ensure that all required data are being recorded with sufficient quality and in a way that makes them analyzable.

Quality control checkpoints. Establishing quality control (QC) checkpoints {cite:p}`Strother2006-np` is necessary for every project: which data are usable for analyses, and which are not? At these key points in the preprocessing or analysis workflow the data’s quality is checked, and if insufficient, it does not move on to the next stage. Low quality data are much less likely to be reproducible with new data or methods. Critically, the exclusion criteria of each checkpoint must be defined in advance (preferably stated in the SOPs and the pre-registration document, see sections {ref}`2.1 <s21>` and {ref}`2.2 <s22>`) to preempt unintentional cherry-picking (i.e., excluding data points to reinforce the results), which is a major contributor to irreproducibility via undisclosed flexibility. Some criteria are widely accepted and applicable, for example, that all neuroimaging data should be screened to eliminate clear artifacts, such as data corrupted by incidental electromagnetic interference or participants movements. A similarly well-established checkpoint of the workflow is visualizing and inspecting the outputs of surface reconstruction methods in MRI, checking time activity curves in high binding regions for PET or power spectral content in MEG and EEG; these fundamental QC checkpoints and their implementation are heavily dependent on the immediately previous processing step. Such QC may be conducted manually by experts using software aids, like visual summary reports or visualization software such as MRIQC {cite:p}`Esteban2017-ow`. More objective, automatic exclusion criteria, are currently an open and active line of work in neuroimaging (e.g., {cite:p}`Ding2019-nw,Esteban2019-nx,Kollada2021-ot`). Some QC checkpoints, such as for acceptable task performance or participant movement, are often defined for individual tasks, experiments and hypotheses.

Quality assurance (QA). Tracking QC decisions will also enable identifying structured failures and artifacts that require not just excluding affected datasets, but rather taking corrective actions to preempt propagation to additional datasets. When a mishap occurs, the experimenters should investigate its cause, and if possible, change the SOPs ({ref}`Section 2.1 <s21>`) and related materials to reduce the chance of it happening again. For example, if many participants report confusion about task instructions, the training procedure and experimenter script could be altered. Automated checks and reports can be very effective, such as real-time monitoring of participant motion during data collection {cite:p}`Heunis2020-ie`, or validating that image parameters are as expected before storage (e.g., with XNAT {cite:p}`Marcus2007-nt` or ReproNim tools {cite:p}`Kennedy2019-jy`).

Positive control analyses. A final aspect of quality assurance is the incorporation of positive control analyses: analyses included not because they are of interest for the scientific questions, but because they provide evidence that the dataset is of sufficient quality to conduct the analyses of interest, and that the analysis is valid. Ideally, positive control analyses focus on strong, well-established effects that must be present if the dataset is valid. For example, with task fMRI designs, button pressing, which should be associated with contralateral motor activation, is often a convenient target for positive control analysis. In MEG and EEG, participants can be asked to blink their eyes, open their mouths, or clench their jaws, and the recordings checked for the associated artifacts. Positive control analyses should also be carried out during piloting, when changes to the protocol are still possible (see {ref}`Section 2.1 <s21>`). For example, if button presses are not clearly detectable during piloting, the acquisition sequence may not have sufficient SNR for the planned analyses and so need modification. Positive controls can further serve for analysis pipeline optimization prior to conducting the optimized analysis on the outcome of interest, thus preventing legitimate optimization from turning into p-hacking.
:::

[^footnote1]: Sources: Icons from the Noun Project: Registration by WEBTECHOPS LLP; Shared by arjuazka; Computer warranty by Thuy Nguyen; Logos: used with permission by the respective copyright holders.

:::{dropdown} References on this page
```{bibliography}
:filter: docname in docnames
:labelprefix: B
```
:::
