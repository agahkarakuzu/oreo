Data processing and analysis
=========================================================

```{admonition} Issue
Researchers typically execute a set of signal pre-processing steps prior to advanced data analysis, to, for instance, identify and remove noise, align data spatially and temporally, segment spatio-temporal regions of interest, identify patterns and latent signal structures (e.g., clustering), integrate the information from several modalities, introduce prior knowledge about the device or the physiology of the specimen, etc. The combination of the operations that take the unprocessed data as the input, prepare the data for analysis, and finally, perform advanced analysis, comprise a full analysis pipeline or workflow. In implementing such analysis workflows, software has emerged as a critical research instrument greatly relevant to ensure the reproducibility of studies.
```

```{admonition} What do we provide
TBE.
```

```{figure} ../figures/fig4.png
---
name: fig4
---
Data processing and analysis [^footnote4].
```

:::{dropdown} {fa}`microscope` 5.1 Software as a research instrument
:class-title: bg-ch5 font-weight-bold
:animate: fade-in
(s51)=
The digital nature of neuroimaging data along with the large, and constantly increasing, net amounts of daily acquired data, place software as a central instrument of the neuroimaging research workflow. As a result, many toolboxes containing utilities ranging from early steps of preprocessing to statistical analysis and visualization of results have emerged, and some have largely shaped the software development in the field, e.g., AFNI {cite:p}`Cox1996-ui,Cox1997-nh`, FSL {cite:p}`Jenkinson2012-az`, SPM {cite:p}`Flandin2008-hr,Litvak2011-ik,Penny2011-nd`, FreeSurfer {cite:p}`Dale1993-cp,Dale1999-ku`, Brainstorm {cite:p}`Tadel2011-ju,Tadel2019-te`, EEGLAB {cite:p}`Delorme2004-ah,Delorme2021-ue`, MNE-Python {cite:p}`Gramfort2013-ok,Gramfort2014-kz`, FieldTrip {cite:p}`Oostenveld2011-qo` (see the [resources table](../09/table.md)). More recently, some software packages have been developed to cover additional aspects of the neuroimaging workflow. For instance, nibabel {cite:p}`Brett2020-hz` to read and write images in many formats, the Advanced Normalization Tools (ANTs) for image registration and segmentation, or Nilearn {cite:p}`Abraham2014-cr` for statistical analysis and visualization. Workflow engines conveniently connect between the building blocks and determine how the steps are executed in the computational environment. Solutions range from general-purpose scripting (e.g., Bash or Python) to neuroimaging-specific libraries (e.g., NiPype; {cite:p}`Gorgolewski2011-nf`). Researchers have all these tools (and others) at their disposal to “mix-and-match” in their workflow. Therefore, ensuring the proper development and operation of the software engine is critical to ensure the reproducibility of results {cite:p}`Tustison2013-rb`).

Relatedly, the variety of software implementations is an additional motive of concern. As remarked by Carp ({cite:p}`Carp2012-kj,Carp2012-zu`) based on the analysis of thousands of fMRI pipelines, analytical flexibility in combination with incomplete reporting precludes the reproducibility of the results. A recent comprehensive investigation, the Neuroimaging Analysis Replication and Prediction Study (NARPS; {cite:p}`Botvinik-Nezer2020-hx`) found that when 70 different teams were asked to analyze the same fMRI data to test the same hypotheses, each team chose a distinct pipeline and results were highly variable. Other studies suggest similar problems in EEG {cite:p}`Clayson2021-nt,Soskic2021-wv`, PET {cite:p}`Norgaard2020-ak` and diffusion MRI {cite:p}`Schilling2021-hd`.

There are two crucial aspects of the high analytical variability and its effect on results in neuroimaging. First, when high analytical variability (that potentially affects results) is combined with partial reporting or with incentives to find significant effects, it can alarmingly undermine the reliability and reproducibility of results. Second, even in the apparently ideal scenario in which the researcher performs a single pre-registered valid analysis and reports it fully and transparently, it is still likely that the results are not robust to arbitrary analytical choices. Therefore, new tools are needed to allow researchers to perform a “multiverse analysis” (section 5.4), where multiple data workflows are used on the same dataset and all the results are reported and their agreement or convergence discussed. Community-led efforts to develop high-quality “gold standard” data workflows may also reduce researchers’ degrees of freedom as well as accelerate data analysis, although different pipelines may be optimal for different research questions and data.

Nevertheless, neuroimaging researchers frequently encounter gaps that readily available toolboxes do not cover. These gaps, amongst a number of other reasons (e.g., deploying a data workflow on a high-performance computer), pushes researchers into creating their own software implementations. However, most neuroimaging researchers are not formally trained in related fields of computer science, data science, or software engineering, and formal software development practices are often not included in undergraduate or graduate level neuroimaging training. This mismatch often results in undocumented, hard to maintain, and disorganized code; largely as a consequence of unawareness of software development practices. It also increases the likelihood of undetected errors that may remain even after running tests on the code.

The first and foremost strategy available to maximize the transparency of research methods is openly sharing the code with the minimal restrictions possible (see {ref}`Section 6.2 <s62>`; {cite:p}`Barnes2010-rl,Gorgolewski2016-iy`). Complementarily, version control systems, such as Git (Blischak, Davenport, and Wilson 2016, see the [resources table](../09/table.md)), are the most basic and effective tool to track how software is developed, and to collaboratively produce code. Beyond making the code available to others, software tools can implement further transparency strategies by thoroughly documenting their tools and by supporting implementations with scientific publications ({cite:p}`Barnes2010-rl,Gorgolewski2016-iy`).
:::


:::{dropdown} {fa}`microchip` 5.2 Standardizing preprocessing and workflows
:class-title: bg-ch5 font-weight-bold
:animate: fade-in
(s52)=
Although the diversity in methodological alternatives has been key to extracting scientific insights from neuroimaging data, appropriately combining heterogeneous tools into complete workflows requires substantial expertise. Traditionally, researchers used default workflows distributed along with individual software packages, or alternatively, individual laboratories have developed in-house analysis workflows that resulted in highly specialized pipelines. Such pipelines are often not thoroughly validated and difficult to reuse due to lack of documentation or accessibility to outside labs. In response, several community-led efforts have spearheaded the development of robust, standardized workflows.

An early effort towards workflow standardization was the Configurable Pipeline for the Analysis of Connectomes (C-PAC; {cite:p}`Craddock2013-xj`), which is a “nose-to-tail” preprocessing and analysis pipeline for resting state fMRI. C-PAC offers a comprehensive configuration file, editable directly with a text editor or through C-PAC’s graphical user-interface, prescribing all the tools and parameters to be executed, and thereby making strides towards keeping methodological decisions closely traced. Similarly, large-scale acquisition initiatives released workflows tailored for their official imaging protocols (e.g., the HCP Pipelines {cite:p}`Glasser2013-qt` and the UK Biobank {cite:p}`Alfaro-Almagro2016-pj`).

Conversely, fMRIPrep {cite:p}`Esteban2019-pm` proposed the alternative approach of restricting the pipeline goals to the preprocessing step, while accepting the maximum diversity possible of the input data (i.e., not tailored to a particular experimental design or analysis-agnosticity). This approach has recently been proposed for additional modalities (e.g., dMRI, ASL, PET) and population/species of interest (e.g., fMRIPrep-rodents, fMRIPrep-infants) under a common framework called NiPreps (NeuroImaging PREProcessing toolS). NiPreps is a community-led endeavor with the goal of ensuring the generalization of the building blocks of preprocessing across modalities (e.g., the alignment of fMRI and dMRI with the same participant / animal’s anatomical image) and specimens (e.g., using the same brain extraction from anatomical data using the same algorithm and implementation on both human adults and rodents). Similar standardization efforts are starting to be adopted for EEG {cite:p}`Desjardins2021-lh` and MEG (e.g., MNE-BIDS pipeline; {cite:p}`Jas2018-qg`). Further examples of standardized workflows are found in the [resources table](../09/table.md).

An additional and relevant premise of standardized workflows is transparency — tools must be transparent not only in their implementation, but also in their reporting. For example, fMRIPrep produces visual reports with the double goal of assessing the quality of results, and also providing the researcher with a resource to comprehensively understand every step of the workflow. In addition, the report includes a text description which comprehensively describes each major step in the pipeline, including the exact software version and principle citation. This text, referred to as the “citation boilerplate”, is released under a public domain license, and therefore can be included verbatim in researcher’s manuscripts, facilitating accurate reporting and proper referencing of academic software. A final relevant aspect towards transparency is the comprehensive documentation of pipelines.

In most cases, standardized workflows preprocess datasets in a fully automated manner, taking a BIDS dataset as input and outputting data that is ready for subsequent analysis with little manual intervention. Importantly, such workflows are typically designed to be as robust as possible to diverse input data (e.g., with varying parameters or sampling distant populations), a challenge that is facilitated by data standardization (i.e., BIDS). Additionally, workflows must be portable, enabling users to execute them in a wide variety of environments. A key technology in this endeavor is containers—such as Docker and Apptainer/Singularity—which facilitate packaging specific versions of heterogeneous dependencies while ensuring cross-platform compatibility (e.g., high-performance computing clusters, desktop, or cloud services). The BIDS apps framework ({ref}`Section 4.1 <s41>`) leverages containers by standardizing input parameters to make it trivially easy to execute a wide variety of standardized workflows on BIDS datasets. An example of a higher-level combination of workflows is found in Esteban et al. {cite:t}`Esteban2020-bc`, which describes an MRI research protocol using MRIQC and fMRIPrep. Finally, recent efforts to standardize the outputs of workflows (BIDS Derivatives), further enhances the interoperability of workflows, by ensuring their outputs are compatible with subsequent analysis.
:::

:::{dropdown} {fa}`calculator` 5.3 Statistical modeling and advanced analysis
:class-title: bg-ch5 font-weight-bold
:animate: fade-in
(s53)=
Analysis of neuroimaging data is particularly heterogeneous and prone to excessive analytical flexibility and underspecified reporting {cite:p}`Carp2012-kj,Carp2012-zu`. Whereas preprocessing is ideally performed once per dataset, there is often a large number of types of analyses that may be used with the preprocessed data. In MRI and fNIRS, for example, analyses range from multi-stage general linear models (GLMs), multivariable decoding analyses, to anatomical and functional connectivity, and more. In PET, analyses consist of region-wise averaging, although voxel-wise approaches are gaining popularity, followed by kinetic modeling and subsequent statistical analyses, which can be GLM or more advanced, such as latent variable models. In MEG and EEG, the broad variety includes analyses such as evoked response potentials, power spectral density, source reconstructions, time-frequency, connectivity, advanced statistics and more. Each type of analysis also has a wide variety of subtypes, parameters, and statistical models that can be specified, and the form of that specification varies across the dozens of analysis packages that implement each type of analysis.

Data analysis reporting may be made more transparent by sharing code that relies on open-source software. A prime example is SPM {cite:p}`Flandin2008-hr`, which has been open source since its inception in 1991. Additional widely used open-source tools for data analysis are FSL and AFNI for MRI, and some examples of reproducible pipelines for MEG and EEG developed based on each of the following software: EEGLAB {cite:p}`Pernet2020-zo`, Fieldtrip {cite:p}`Andersen2018-re,Meyer2021-ze,Popov2018-uu`, Brainstorm {cite:p}`Niso2019-nr,Tadel2019-te`, SPM {cite:p}`Henson2019-hc` and MNE-Python {cite:p}`Andersen2018-un,Jas2018-qg,Van_Vliet2018-ob` (see {cite:p}`Niso2022-ng` for a detailed review on main EEG and MEG open toolboxes and reproducible pipelines). Reproducibility is also improved when relying on modular and well-documented software such as Nilearn, which offers versatile methods to perform advanced analyses of fMRI data, from GLM to connectomic and machine learning {cite:p}`Abraham2014-cr`. Ideally, a single analysis script is made creating a report, from signal extraction, data analysis, and reproducing all figures.

An additional challenge for the reproducibility of analysis workflows is the representation of statistical models across distinct implementations of analysis software. For example, GLM approaches to analyze fMRI time series are prevalent and supported by all of the major statistical packages (e.g., AFNI, SPM, FSL, Nilearn). However, specifying equivalent models across packages is non-trivial and requires time consuming package specific model specification {cite:p}`Pernet2014-xg`, which obfuscates details of the statistical model, exacerbates variability across pipelines, and makes it difficult to perform multiverse analyses (see {ref}`Section 5.4 <s54>`). The BIDS Stats Model (BIDS-SM, see the [resources table](../09/table.md)) specification has been proposed as a implementation-independent representation of fMRI GLM models; BIDS-SM describes the inputs, steps, and specification details of GLM-type analyses, and encodes them in a machine readable JSON format. The PyBIDS library provides tooling to facilitate reading BIDS-SM, and FitLins {cite:p}`Markiewicz2021-yr` is a reference workflow that fits BIDS-SM using AFNI or Nilearn. The transformative potential of BIDS-SM is showcased by Neuroscout {cite:p}`De_la_Vega2022-xo`, a turnkey platform for fast and flexible neuroimaging analysis. Neuroscout provides a user-friendly web application for creating BIDS-SM on a curated set of public neuroimaging datasets, and leverages FitLins to fit statistical models in a fully reproducible and portable workflow. By standardizing the entire process of statistical modeling, users can formally specify a hypothesis and produce statistical results in a matter of minutes, while simultaneously ensuring a fully reproducible and transparent analysis that can be readily disseminated to the scientific community.
:::


:::{dropdown} {fa}`globe` 5.4 Multiverse analysis
:class-title: bg-ch5 font-weight-bold
:animate: fade-in
(s54)=
The variety of data workflows reflects the enormous interest and the need for novel software instruments, but it also poses an important risk to reproducibility. The multitude of possible combinations of methods and parameters in each of the analysis steps creates an extremely large number of combinations to select from. This problem is often referred to as “researcher degrees of freedom” or “the garden of forking paths” {cite:p}`Gelman2013-dw`. Importantly, analytical choices affect results. This has been shown for preprocessing of fMRI data already back in 2004 {cite:p}`Strother2004-nk`. More work in this direction followed in 2012 {cite:p}`Churchill2012-pi,Churchill2012-sr`. While this work focused mainly on the aspect of tailoring preprocessing to e.g. maximize predictive models, recent efforts in fMRI (task fMRI: {cite:p}`Botvinik-Nezer2020-hx,Carp2012-zu`; preprocessing of resting-state fMRI: {cite:p}`Li2021-pk`) and PET (specifically for preprocessing: {cite:p}`Norgaard2020-ak`) focused more on the variability of outcomes in general when analysis pipelines were varied. In addition, recent studies showed high variability in diffusion-based tractography dissection {cite:p}`Schilling2021-hd` and event-related potentials in EEG preprocessing {cite:p}`Clayson2021-nt,Soskic2021-wv`. Another large-scale attempt to estimate the analytical variability for EEG,  EEGManyPipelines (see the [resources table](../09/table.md)), is currently ongoing.

The converging findings of these studies across modalities suggest that it is important to test the robustness of reported results to specific analytical choices. One proposed solution to tackle the analytical variability, where many different analytical approaches are compared, is multiverse analysis {cite:p}`Hall2022-hn`. There are two broad types of multiverse tools. In a “numerical instabilities” approach, different setups and numerical errors or uncertainties in computational tools are evaluated, analyses are rerun several times, and variability, robustness, and “mean answer” are estimated {cite:p}`Kiar2020-bh`. One tool of this type that is being developed is “Fuzzy” {cite:p}`Kiar2021-cw`. Alternatively, in a “classic multiverse analysis”, multiple pipelines are used with the same data and the results are compared across pipelines. Such an analysis could be conducted by a single or by multiple researchers {cite:p}`Aczel2021-hu`. Although multiverse analysis was suggested before in other fields {cite:p}`Patel2015-nm,Simonsohn2015-lm,Steegen2016-cr`, there are not yet mature “classic multiverse analysis” tools for high-dimensional data like in neuroimaging. Explorable Multiverse Analyses is an R-tool that allows the readers to explore different statistical approaches in a paper {cite:p}`Dragicevic2019-mj`. Other tools, such as the Python-based Boba {cite:p}`Liu2021-up`, aim to facilitate multiverse analyses by allowing users to specify the shared and the varying parts of the code only once and by providing useful visualizations of the pipelines and results. However, these tools currently fit simpler analyses and datasets compared to the ones common in neuroimaging.

In neuroimaging, recent progress has been made in creating infrastructure for multiverse analysis in fMRI, based on the C-PAC tool (see {ref}`Section 5.2 <s52>`; {cite:p}`Li2021-pk`). Ongoing efforts to formalize machine-readable standards for statistical models (BIDS-SM) and pipelines to estimate them, and their integration with datasets using platforms such as brainlife.io {cite:p}`Avesani2019-vi`, could facilitate the development of multiverse tools. In order to make sense of a multiverse analysis, one needs methods to test for convergence across results of diverse analysis pipelines with the same data. Such a method for fMRI image-based meta-analysis was recently used in NARPS {cite:p}`Botvinik-Nezer2020-hx` as well as in subsequent projects {cite:p}`Bowring2021-pw`. Another simple statistical approach to a multiverse analysis was presented with PET data {cite:p}`Norgaard2019-su`, although it  lacks statistical power, due to the use of a very conservative statistic. A different approach is to use active learning to approximate the whole multiverse space {cite:p}`Dafflon2020-cn`. Moreover, Boos et al. {cite:t}{Boos2021-az} provided an online application to explore the effects of the choice of parameters on the results (data-driven auditory encoding, see the [resources table](../09/table.md)). Progress is still needed until such tools are mature enough to allow scalable multiverse analysis in neuroimaging.
:::

[^footnote4]: Sources: Icons from the Noun Project: Software by Adrien Coquet; Workflow by D. Sahua; Statistics by Creative Stall; Chaos Sigil by Avana Vana; Logos: used with permission by the copyright holders.

:::{dropdown} References on this page
```{bibliography}
:filter: docname in docnames
:labelprefix: E
```
:::
